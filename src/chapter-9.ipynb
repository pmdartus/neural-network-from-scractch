{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 - Backpropagation\n",
    "\n",
    "The motivation behind backpropagation is find how each paramater (weight and baises) affects the loss function to minimize it.\n",
    "\n",
    "Effectively a model can be expressed as a set of composite function, where $y$ is the function output.\n",
    "\n",
    "$$\n",
    "y = f_3(f_2(f_1(x)))\n",
    "$$\n",
    "\n",
    "Because of the chain rule we can express the loss function ($L$) partial derivation as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial f_3}.\\frac{\\partial f_3}{\\partial f_2}.\\frac{\\partial f_2}{\\partial f_1}.\\frac{\\partial f_1}{\\partial x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single neuron\n",
    "\n",
    "### Forward propagation\n",
    "\n",
    "In this example, we have a single neuron with 2 parameters + a ReLU activation function:\n",
    "\n",
    "$$\n",
    "z = w_1.x_1 + w_2.x_2 + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = ReLU(z) = max(z, 0)\n",
    "$$\n",
    "\n",
    "* $x_1$, $x_2$: input values\n",
    "* $w_1$, $w_2$: neuron weights\n",
    "* $b$: neuron bais\n",
    "\n",
    "The loss function ($L$) is the mean squared error, aka MSE. Where $t$ is a target value:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2}(y-t)^2\n",
    "$$\n",
    "\n",
    "### Back propagation\n",
    "\n",
    "Step 1: Compute loss function partial derivative\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y} = y - t\n",
    "$$\n",
    "\n",
    "Step 2: Compute activation function partial derivative\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial z}= \\begin{cases}\n",
    "   1 &\\text{if } z > 0 \\\\\n",
    "   0 &\\text{otherwise }\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Step 3: Compute neuron partial derivatives\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial w_1} = x_1, \\quad \\frac{\\partial z}{\\partial w_2} = x_2, \\quad \\frac{\\partial z}{\\partial b} = 1\n",
    "$$\n",
    "\n",
    "Step 4: Apply chain rule to compute the partial derivative (assuming ReLU is activated)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_1} = (y - t) . 1 . x_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_2} = (y - t) . 1 . x_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = (y - t) . 1 . 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the intial inputs and neurons parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1.0, -2.0, 3.0] # Inputs\n",
    "w = [-3.0, -1.0, 2.0] # Weights\n",
    "b = 1.0 # Bais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply forward propagations. For this example we will optimize the neural network output, by making it converge to 0, which isn't a realistic example. In practice the goal is to optimize the loss function, not the neural network output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 6.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate neuron output pre-activation\n",
    "xw0 = w[0] * x[0]\n",
    "xw1 = w[1] * x[1]\n",
    "xw2 = w[2] * x[2]\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# Calculate neuron output by applying ReLU\n",
    "y = max(0, z)\n",
    "\n",
    "print(\"Output:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply derivation starting from the end activation function. Using the `dx_dy` variable naming convention to denote the partial derivative of `x` over `y`: $\\frac{\\partial x}{\\partial y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function derivative.\n",
    "dy_dz = (1 if z > 0 else 0)\n",
    "\n",
    "# Neuron derivative pre-activation. \n",
    "dz_dw0 = x[0]\n",
    "dz_dw1 = x[1]\n",
    "dz_dw2 = x[2]\n",
    "dz_db = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight gradient: [1.0, -2.0, 3.0]\n",
      "Bais gradient: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Apply chain rule to caculate activation function derivative relative to \n",
    "# weights and bais.\n",
    "dy_dw0 = dy_dz * dz_dw0\n",
    "dy_dw1 = dy_dz * dz_dw1\n",
    "dy_dw2 = dy_dz * dz_dw2\n",
    "dy_db = dy_dz * dz_db\n",
    "\n",
    "dw = [dy_dw0, dy_dw1, dy_dw2] # Weights gradient\n",
    "db = dy_db # Bais gradient\n",
    "\n",
    "print(\"Weight gradient:\", dw)\n",
    "print(\"Bais gradient:\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the gradient to make slight modifications to the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = -0.01\n",
    "\n",
    "w[0] += dw[0] * epsilon\n",
    "w[1] += dw[1] * epsilon\n",
    "w[2] += dw[2] * epsilon\n",
    "b += db * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-running the same forward pass with the adjusted weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 5.8500000000000005\n"
     ]
    }
   ],
   "source": [
    "# Calculate neuron output pre-activation\n",
    "xw0 = w[0] * x[0]\n",
    "xw1 = w[1] * x[1]\n",
    "xw2 = w[2] * x[2]\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# Calculate neuron output by applying ReLU\n",
    "y = max(0, z)\n",
    "\n",
    "print(\"Output:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after adjusting the parameters, the output when from `6.00` to `5.85`. Yeah!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
